# -*- coding: utf-8 -*-
"""JuanSource Ollama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k7GupfbQ_adVlM7W8HwQzQq7Nf8BcdLG
"""

# Cell 1: Install Dependencies and Ollama Setup

# Install Python Libraries (LangChain and Google Search)
# !pip install -q langchain langchain-ollama langchain-community google-search-results

# Install the necessary networking tools for the local server
# !pip install -q nest-asyncio requests

# --- Ollama Setup (Running the server inside Colab) ---
# 1. Download and install the Ollama binary
#!curl -fsSL https://ollama.com/install.sh | sh

# 2. Start the Ollama server in the background
import subprocess
import time
import os

# Set environment variable to allow Ollama to run inside Colab's sandbox
os.environ['OLLAMA_HOST'] = '0.0.0.0'
subprocess.Popen(["ollama", "serve"])

print("Ollama server starting...")
time.sleep(10) # Give the Ollama server 10 seconds to fully initialize

# 3. Pull the specific model you want to use (e.g., Llama 3.1)
# You should choose a small, fast model for free Colab CPU (like phi3 or llama2)
OLLAMA_MODEL = "llama2" # <<< We will use llama2, but you can change this

print(f"Pulling {OLLAMA_MODEL}...")
!ollama pull $OLLAMA_MODEL

print("Ollama setup complete!")

# Cell 2: Securely Handle Google Search API Keys

from getpass import getpass

# --- GOOGLE CUSTOM SEARCH API Keys (for the Retrieval Tool) ---
# You need: 1. API Key (from Google Cloud Console) and 2. Search Engine ID (CX)
os.environ["GOOGLE_API_KEY"] = input("Insert Google API Key here")
os.environ["GOOGLE_CSE_ID"] = input("Insert Google CSE API Key here")

print("\nGoogle Search keys loaded securely.")

# Cell 3: Define the RAG Components

from langchain_community.chat_models import ChatOllama # <<< NEW LLM IMPORT
from langchain_community.utilities import GoogleSearchAPIWrapper
from langchain_core.tools import Tool
from langchain.prompts import PromptTemplate

# --- I. The Reasoning Engine (LLM) ---
# Note: The base_url points to the local server started in Cell 1
llm = ChatOllama(
    model=OLLAMA_MODEL,
    base_url="http://localhost:11434", # Default Ollama local address
    temperature=0.1,
)

# --- II. The Retrieval Tool (Google Search) ---
search = GoogleSearchAPIWrapper()
google_search_tool = Tool(
    name="Google Search",
    description="A tool for retrieving real-time, external facts, and evidence from the internet.",
    func=lambda query: search.results(query, num_results=10)
)

# --- III. The Reasoning Prompt Template ---
# CRITICAL: We instruct the LLM to return ONLY a JSON string.
RAG_PROMPT_TEMPLATE = """
**FACT-CHECKER ASSIGNMENT: RAG Fake News Detector**

You are an objective, expert fact-checker. Your task is to analyze a user's query against
the real-time evidence retrieved from Google Search.

**1. QUERY/CLAIM TO VERIFY:**
{query}

**2. RETRIEVED EVIDENCE (Search Results):**
---
{search_results}
---

**INSTRUCTIONS FOR REASONING:**
A. **Classification:** Determine the veracity of the QUERY. Output MUST be either **REAL** or **FAKE**.
B. **Reasoning:** Explain your decision, citing key points from the RETRIEVED EVIDENCE.
C. **Evidence Sourcing:** Extract only the unique, full URLs (links) from the RETRIEVED EVIDENCE that directly support your conclusion.

**FINAL OUTPUT FORMAT:**
Your response MUST be a single, valid JSON object following this exact schema. DO NOT include any extra text or markdown fences:
{{"classification": "[REAL or FAKE]", "reasoning": "[Detailed, evidence-based explanation]", "evidence_links": ["url_1", "url_2", "..."]}}
"""

def fake_news_detector_rag(claim: str):
    """Core RAG function that runs the fact-check."""

    print("2. Performing live Google Search for evidence...")
    try:
        search_results = google_search_tool.run(claim)
    except Exception as e:
        return f"Error during Google Search: {e}"

    final_prompt = RAG_PROMPT_TEMPLATE.format(
        query=claim,
        search_results=search_results
    )

    print("3. Sending evidence to Ollama for Reasoning...")
    try:
        # Pass the augmented prompt to the local Ollama LLM
        response = llm.invoke(final_prompt)
        return response.content
    except Exception as e:
        return f"Error during Ollama Reasoning: {e}. Is the Ollama server still running?"

# Cell 4: Execute Test with a sample claim

import json
import re

test_claim = "K to 12 will be removed in the Philippines"

print("=" * 60)
print(f"**Analyzing Query:** {test_claim}")

result_str = fake_news_detector_rag(test_claim)

print("\n--- FINAL DETECTOR OUTPUT ---")

# We attempt to clean and pretty-print the JSON output
try:
    # Remove markdown fences and whitespace
    clean_json_str = re.sub(r'```(?:json)?', '', result_str, flags=re.IGNORECASE).strip()

    # Try loading as JSON
    result_dict = json.loads(clean_json_str)
    print(json.dumps(result_dict, indent=4))
except json.JSONDecodeError:
    # If the LLM failed to output perfect JSON, print the raw string
    print("WARNING: Could not parse JSON. Raw LLM Output:")
    print(result_str)

print("=" * 60)